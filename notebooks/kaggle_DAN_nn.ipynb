{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../src/\")\n",
    "sys.path.append(\"../model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "from nn_model import fully_connected_embed, embed_nn\n",
    "from io_utils import load_dataset, load_model, model_log\n",
    "from metric import performance_logloss, performance_pr_auc\n",
    "from train_utils import extend_dataset, reduce_dataset, sample_validation_data\n",
    "\n",
    "from labelshift_correction import build_pivot_dataset, adjust_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"dan\"\n",
    "\n",
    "task = \"kaggle\" # the dataset that we are working on\n",
    "data_type = \"cate\" # the type of data that we are dealing with\n",
    "num_dim = 43\n",
    "epoch = 25\n",
    "batch_size = 1024\n",
    "period = [0, 1, 2, 3] # the period of data\n",
    "cate_index = 8 # the index of the last categorical feature\n",
    "version = \"opt\" # the version of embedding matrix & prediction model\n",
    "source_domain = \"source\"\n",
    "target_domain = \"target\"\n",
    "\n",
    "device = torch.device(\"cuda\") # device of training \n",
    "        \n",
    "embedding_input = [3, 131, 4, 483, 103, 5, 106, 4] # different levels of categorical features\n",
    "embedding_dim = [1, 3, 1, 4, 3, 1, 3, 1] # embedding dimension\n",
    "\n",
    "ratio = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameter selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "beta_range = [0.005, 0.01, 0.1]\n",
    "period_beta = []\n",
    "\n",
    "for p in period:\n",
    "    perf_beta = []\n",
    "    for beta in beta_range:\n",
    "        perfs = []\n",
    "        for seed in range(10):\n",
    "            torch.manual_seed(seed)\n",
    "            np.random.seed(seed)\n",
    "\n",
    "            source_train, source_train_label, source_test, source_test_label = load_dataset(\"../data/\", \n",
    "                                                                                            task, source_domain, data_type, p)\n",
    "            target_train, target_train_label, target_test, target_test_label = load_dataset(\"../data/\", \n",
    "                                                                                            task, target_domain, data_type, p)\n",
    "\n",
    "            # get target_factor and source_factor\n",
    "            target_factor = (target_train_label[:, 1]==0).sum() / target_train_label[:, 1].sum()\n",
    "            source_factor = (source_train_label[:, 1]==0).sum() / source_train_label[:, 1].sum()\n",
    "\n",
    "            # adjusting source train dataset\n",
    "            source_train, source_train_label, source_index = build_pivot_dataset(\n",
    "                source_train, source_train_label[:,1], target_factor, source_factor)\n",
    "            \n",
    "            # train undersample\n",
    "            valid_index, source_train_label = sample_validation_data(task, source_train_label, ratio)\n",
    "            source_train = source_train[valid_index]\n",
    "            \n",
    "            valid_index, target_train_label = sample_validation_data(task, target_train_label, ratio)\n",
    "            target_train = target_train[valid_index]\n",
    "\n",
    "            source_train, source_valid, source_train_label, source_valid_label = train_test_split(\n",
    "                source_train, source_train_label, test_size=0.25, shuffle=False, random_state=0)\n",
    "\n",
    "            embed = embed_nn(embedding_input, embedding_dim, num_dim)\n",
    "            model = fully_connected_embed(embed, cate_index, device)\n",
    "\n",
    "            source_index, target_index = reduce_dataset(source_train, target_train)\n",
    "\n",
    "            model.fit(source_train[source_index], source_train_label[source_index], \n",
    "                      target_train[target_index],\n",
    "                      source_valid, source_valid_label, \n",
    "                      epoch=epoch, batch_size=batch_size, lr=0.005, beta=beta,\n",
    "                      early_stop=False, verbose=False)\n",
    "\n",
    "            pred = model.predict(source_test)\n",
    "            perf = performance_logloss(pred, source_test_label[:, 1])\n",
    "            perfs.append(perf)\n",
    "        perf_beta.append(np.mean(perfs))\n",
    "    period_beta.append(beta_range[np.argmax(perf_beta)])\n",
    "\n",
    "path = os.path.join(\"./results\", task, model_type, \n",
    "                     \"{}_{}\".format(model_type, version))\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "np.save(os.path.join(path, \"period_beta\"), period_beta)\n",
    "\n",
    "print(\"Optimal beta for each period:\", period_beta, flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in period:\n",
    "    beta = period_beta[p]\n",
    "    for seed in range(10):\n",
    "        print(\"Period\", p, \"Seed\", seed, flush=True)\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        source_train, source_train_label, source_test, source_test_label = load_dataset(\"../data/\", \n",
    "                                                                                        task, source_domain, data_type, p)\n",
    "        target_train, target_train_label, target_test, target_test_label = load_dataset(\"../data/\", \n",
    "                                                                                        task, target_domain, data_type, p)\n",
    "\n",
    "        # get target_factor and source_factor\n",
    "        target_factor = (target_train_label[:, 1]==0).sum() / target_train_label[:, 1].sum()\n",
    "        source_factor = (source_train_label[:, 1]==0).sum() / source_train_label[:, 1].sum()\n",
    "\n",
    "        # adjusting source train dataset\n",
    "        source_train, source_train_label, source_index = build_pivot_dataset(\n",
    "            source_train, source_train_label[:,1], target_factor, source_factor)\n",
    "\n",
    "        source_train, source_valid, source_train_label, source_valid_label = train_test_split(\n",
    "            source_train, source_train_label, test_size=0.25, shuffle=False, random_state=0)\n",
    "\n",
    "        embed = embed_nn(embedding_input, embedding_dim, num_dim)\n",
    "        model = fully_connected_embed(embed, cate_index, device)\n",
    "\n",
    "        source_index, target_index = reduce_dataset(source_train, target_train)\n",
    "\n",
    "        model.fit(source_train[source_index], source_train_label[source_index, 1], \n",
    "                  target_train[target_index],\n",
    "                  source_valid, source_valid_label[:, 1], \n",
    "                  epoch=epoch, batch_size=batch_size, lr=0.005, beta=beta,\n",
    "                  early_stop=False, verbose=False)\n",
    "        \n",
    "        path = os.path.join(\"./results\", task, model_type, \n",
    "                     \"{}_{}\".format(model_type, version), \n",
    "                     \"period{}\".format(p), \"exp{}\".format(seed))\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "\n",
    "\n",
    "        # source prediction\n",
    "        pred = model.predict(source_test)\n",
    "        np.save(os.path.join(path, \"source_test_pred\"), pred.astype(np.float16))\n",
    "\n",
    "        perf = performance_logloss(pred, source_test_label[:, 1])\n",
    "        print(\"Source Prediction logloss:\", perf, flush=True)\n",
    "        model_log(\"../logs/logloss\", task, source_domain, \"nn\", p, version, \n",
    "                     \"source_{}: {}\".format(model_type, perf))\n",
    "\n",
    "        perf = performance_pr_auc(pred, source_test_label[:, 1])\n",
    "        print(\"Source Prediction pr_auc:\", perf, flush=True)\n",
    "        model_log(\"../logs/pr_auc\", task, source_domain, \"nn\", p, version, \n",
    "                     \"source_{}: {}\".format(model_type, perf))\n",
    "\n",
    "\n",
    "        # target prediction\n",
    "        pred = model.predict(target_test)\n",
    "        np.save(os.path.join(path, \"target_test_pred\"), pred.astype(np.float16))\n",
    "\n",
    "        perf = performance_logloss(pred, target_test_label[:, 1])\n",
    "        print(\"Target Prediction logloss:\", perf, flush=True)\n",
    "        model_log(\"../logs/logloss\", task, source_domain, \"nn\", p, version, \n",
    "                     \"target_{}: {}\".format(model_type, perf))\n",
    "\n",
    "        perf = performance_pr_auc(pred, target_test_label[:, 1])\n",
    "        print(\"Target Prediction pr_auc:\", perf, flush=True)\n",
    "        model_log(\"../logs/pr_auc\", task, source_domain, \"nn\", p, version, \n",
    "                     \"target_{}: {}\".format(model_type, perf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
