{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../src/\")\n",
    "sys.path.append(\"../model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from io_utils import load_dataset, load_model, model_log\n",
    "from metric import performance_logloss, performance_pr_auc\n",
    "\n",
    "from coordinate_ot_adaptation import adaptation\n",
    "from labelshift_correction import build_pivot_dataset, adjust_model\n",
    "from prediction_stable_feature_selection import scda\n",
    "from train_utils import sample_validation_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"nn\" # for NN models\n",
    "# model_type = \"lgb\" # for GBDT models\n",
    "\n",
    "# number of threads\n",
    "njobs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "source_version = \"opt\" # the version of embedding matrix & prediction model that we use\n",
    "task = \"kaggle\"\n",
    "data_type = \"cate\"\n",
    "num_dim = 43\n",
    "period = [0, 1, 2, 3]\n",
    "cate_index = 8\n",
    "source_domain = \"source\"\n",
    "target_domain = \"target\"\n",
    "ratio = 0.2\n",
    "version = \"scda\"\n",
    "\n",
    "num_selected_features = [[] for _ in period]\n",
    "\n",
    "for seed in range(10):\n",
    "    for p in period:\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        print(\"Period:\", p, seed, flush=True)\n",
    "        model = load_model(\"../model/\", task, source_domain, model_type, p, source_version)\n",
    "        source_train, source_train_label, source_test, source_test_label = load_dataset(\"../data/\", \n",
    "                                                                                        task, source_domain, data_type, p)\n",
    "        target_train, target_train_label, target_test, target_test_label = load_dataset(\"../data/\", \n",
    "                                                                                        task, target_domain, data_type, p)\n",
    "\n",
    "        # get target_factor and source_factor\n",
    "        target_factor = (target_train_label[:, 1]==0).sum() / target_train_label[:, 1].sum()\n",
    "        source_factor = (source_train_label[:, 1]==0).sum() / source_train_label[:, 1].sum()\n",
    "\n",
    "        # adjusting the classifier\n",
    "        model = adjust_model(model, target_factor, source_factor)\n",
    "\n",
    "        # adjusting source train dataset\n",
    "        source_train, source_train_label, source_index = build_pivot_dataset(\n",
    "            source_train, source_train_label[:,1], target_factor, source_factor)\n",
    "\n",
    "        # source and target datat undersampling\n",
    "        source_train_index, _ = sample_validation_data(task, source_train_label, ratio)\n",
    "        source_train = source_train[source_train_index]\n",
    "        \n",
    "        target_train_index, _ = sample_validation_data(task, target_train_label, ratio)\n",
    "        target_train = target_train[target_train_index]\n",
    "        \n",
    "        # source train prediction basedline\n",
    "        pred_source = model.predict(source_train)\n",
    "\n",
    "        # init adaptation & fit & transform\n",
    "        adapt = adaptation(cate_dim=cate_index, num_dim=num_dim)\n",
    "        adapt.fit(target_train, source_train, lmbda=1e-1)\n",
    "\n",
    "        target_train_trans = adapt.transform(target_train, repeat=5, njobs=njobs)\n",
    "\n",
    "        params = {\n",
    "            \"model\": model, \n",
    "            \"valid\": target_train,\n",
    "            \"valid_trans\": target_train_trans,\n",
    "            \"valid_label\": pred_source, \n",
    "            \"cate_index\": cate_index, \n",
    "            \"repeat\": 5,\n",
    "            \"feature_cluster\":[[i] for i in range(target_train.shape[-1])],\n",
    "            \"bias_tol_range\": np.power(10, np.linspace(np.log10(0.005), np.log10(0.2), 15)),\n",
    "            \"best\": None,\n",
    "            \"feature_mask\": None,\n",
    "            \"n_bootstrap\": 100,\n",
    "            \"verbose\": False,\n",
    "        }\n",
    "\n",
    "        path = os.path.join(\"./results\", task, version, \n",
    "                     \"{}_{}\".format(model_type, source_version), \n",
    "                     \"period{}\".format(p), \"exp{}\".format(seed))\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        \n",
    "        # greedy feature selection\n",
    "        feature_mask, best_history = scda(**params)\n",
    "        np.save(os.path.join(path, \"feature_mask\"), feature_mask)\n",
    "        num_selected_features[p].append(feature_mask.sum())\n",
    "        \n",
    "        # target test transformation based on selected features\n",
    "        target_test_trans = adapt.transform(target_test, repeat=20, interpolation=feature_mask, njobs=njobs)\n",
    "\n",
    "        pred = model.predict(target_test_trans).reshape(20, -1).mean(axis=0)\n",
    "        np.save(os.path.join(path, \"target_test_pred\"), pred)\n",
    "\n",
    "        perf = performance_pr_auc(pred, target_test_label[:, 1])\n",
    "        model_log(\"../logs/pr_auc/\", task, source_domain, model_type, p, source_version, \n",
    "                 \"{}: {}\".format(version, perf))\n",
    "        print(\"Target Prediction pr_auc\", perf, flush=True)\n",
    "\n",
    "        perf = performance_logloss(pred, target_test_label[:, 1])\n",
    "        model_log(\"../logs/logloss/\", task, source_domain, model_type, p, source_version, \n",
    "                 \"{}: {}\".format(version, perf))\n",
    "        print(\"Target Prediction logloss\", perf, flush=True)\n",
    "\n",
    "num_selected_features = np.array(num_selected_features)\n",
    "path = os.path.join(\"./results\", task, version, \n",
    "                     \"{}_{}\".format(model_type, source_version))\n",
    "np.save(os.path.join(path, \"num_selected_features\"), num_selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Number of Selected Feature:\", num_selected_features.mean(axis=1))\n",
    "# print(\"Std of Selected Feature:\", num_selected_features.std(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
